# üß™ Templates pour les Tests Automatis√©s

## üü¢ Template D√©butant : Tests Unitaires Simples

```
Cr√©e une suite de tests unitaires pour [FONCTION/MODULE] en [LANGAGE].

FONCTION √Ä TESTER :
[COLLER_LE_CODE_DE_LA_FONCTION]

CAS DE TEST √Ä COUVRIR :
1. [CAS_NOMINAL : description, entr√©e, sortie attendue]
2. [CAS_ERREUR : description, entr√©e, exception attendue]
3. [CAS_LIMITE : description, entr√©e edge case, comportement attendu]

FRAMEWORK DE TEST :
- [LANGAGE] : [JUnit, pytest, Jest, etc.]
- Assertions : [biblioth√®que utilis√©e]
- Mocking : [si n√©cessaire pour d√©pendances externes]

STRUCTURE DES TESTS :
- Arrange : pr√©paration donn√©es et objets
- Act : ex√©cution fonction test√©e
- Assert : v√©rification r√©sultats

G√âN√âRATION DEMAND√âE :
- Code complet des tests
- Ex√©cution et r√©sultats attendus
- Explication de chaque test case
```

## üü° Template Interm√©diaire : Tests d'Int√©gration

```
D√©veloppe des tests d'int√©gration pour [MODULE/SERVICE] avec [D√âPENDANCES_EXTERNES].

CONTEXTE TECHNIQUE :
- Composant test√© : [DESCRIPTION_MODULE]
- D√©pendances : [API, BASE_DONN√âES, SERVICES_EXTERNES]
- Environnement : [DEV, STAGING, PRODUCTION-LIKE]

STRAT√âGIE DE TEST :
APPROCHE :
- [BIG_BANG, TOP_DOWN, BOTTOM_UP, SANDWICH]
- Isolation : [CONTAINERS, MOCKS, STUBS]
- Donn√©es de test : [FIXTURES, FACTORIES, RANDOM]

SC√âNARIOS D'INT√âGRATION :
1. [SCENARIO_1 : description compl√®te]
   - Pr√©conditions : [√©tat initial requis]
   - Actions : [s√©quence d'appels/interactions]
   - Assertions : [r√©sultats attendus]

2. [SCENARIO_2 : description compl√®te]
   - Pr√©conditions : [√©tat initial requis]
   - Actions : [s√©quence d'appels/interactions]
   - Assertions : [r√©sultats attendus]

INFRASTRUCTURE DE TEST :
- Testcontainers : [DOCKER_IMAGES_UTILIS√âES]
- Base de donn√©es : [IN_MEMORY_DB, REAL_DB_ISOLATED]
- Services externes : [WIREFMOCK, CONTRACT_TESTING]
- CI/CD : [PIPELINE_INTEGRATION]

CODES DE TEST :
- Setup/teardown automatis√©
- Gestion des donn√©es de test
- Assertions multi-niveaux
- Reporting d√©taill√©
```

## üî¥ Template Avanc√© : Tests End-to-End Complets

```
Con√ßois une strat√©gie compl√®te de tests end-to-end pour [APPLICATION_WEB/MOBILE/API].

APPLICATION √Ä TESTER :
- Type : [WEB_APP, MOBILE_APP, API_REST, MICROSERVICES]
- Utilisateurs : [PROFILS_UTILISATEUR_TYPES]
- Parcours critiques : [USER_JOURNEYS_PRINCIPAUX]

ARCHITECTURE DE TEST :
OUTILS TECHNIQUES :
- Framework E2E : [Cypress, Selenium, Playwright, Appium]
- Langage : [JavaScript, Python, Java, C#]
- CI/CD : [GitHub Actions, Jenkins, GitLab CI]
- Reporting : [Allure, ReportPortal, TestRail]

STRAT√âGIE DE COUVERTURE :
USER JOURNEYS CRITIQUES :
1. [PARCOURS_1 : description compl√®te]
   - √âtapes : [CLIC, SAISIE, NAVIGATION]
   - Assertions : [√âL√âMENTS_VISIBLES, DONN√âES_CORRECTES]
   - Donn√©es de test : [SC√âNARIOS_VARIATIONS]

2. [PARCOURS_2 : description compl√®te]
   - √âtapes : [CLIC, SAISIE, NAVIGATION]
   - Assertions : [√âL√âMENTS_VISIBLES, DONN√âES_CORRECTES]
   - Donn√©es de test : [SC√âNARIOS_VARIATIONS]

GESTION DES DONN√âES :
- Isolation : [TEST_DATA_SEPARATION, CLEANUP_AUTOMATIQUE]
- Variabilit√© : [RANDOM_DATA_GENERATION, EDGE_CASES]
- Persistance : [DATABASE_RESET, API_MOCKING]

ENVIRONNEMENTS DE TEST :
- Local : [DOCKER_COMPOSE, LOCALSTACK]
- CI : [PARALLEL_EXECUTION, MATRIX_TESTING]
- Staging : [PRODUCTION_LIKE_ENVIRONMENT]
- Production : [CANARY_TESTING, FEATURE_FLAGS]

PERFORMANCE E2E :
- M√©triques : [RESPONSE_TIME, MEMORY_USAGE, ERROR_RATE]
- Thresholds : [ACCEPTABLE_LIMITS_PAR_M√âTRIQUE]
- Monitoring : [REAL_TIME_ALERTS, HISTORICAL_TRENDS]

R√âSILIENCE ET CHAOS :
- Network failures simulation
- Service degradation testing
- Database connection loss
- High load scenarios

MAINTENANCE DES TESTS :
- Flakiness detection : [RETRY_MECHANISMS, STABILITY_METRICS]
- Refactoring : [PAGE_OBJECTS, CUSTOM_COMMANDS]
- Documentation : [TEST_CASE_DOCUMENTATION, VIDEO_RECORDING]
```

## üöÄ Template Expert : Pyramide de Tests Compl√®te

```
Architecte une pyramide de tests compl√®te pour [APPLICATION_ENTERPRISE] avec automatisation maximale.

CONTEXTE APPLICATION :
- √âchelle : [TAILLE_CODEBASE, √âQUIPES, UTILISATEURS]
- Complexit√© : [MICROSERVICES, L√âGACY_SYSTEMS, DISTRIBUTED_ARCHITECTURE]
- Criticit√© : [BUSINESS_CRITICAL, REGULATED_INDUSTRY]

PYRAMIDE DE TESTS CIBLE :
UNITAIRES (70-80%) :
- Couverture : > 80% lignes de code
- Rapidit√© : < 10 minutes ex√©cution compl√®te
- Isolation : tests ind√©pendants, mocks/stubs
- Frameworks : [JUnit, pytest, Jest, xUnit]

INTEGRATION (15-20%) :
- Couverture : APIs, bases de donn√©es, services externes
- Rapidit√© : < 30 minutes
- Environnements : containers, test databases
- Patterns : [CONSUMER_DRIVEN_CONTRACTS, CDC]

END-TO-END (5-10%) :
- Couverture : user journeys critiques uniquement
- Rapidit√© : < 60 minutes
- Environnements : staging/production-like
- Focus : business value vs technical coverage

STRAT√âGIE D'AUTOMATISATION :
INFRASTRUCTURE CI/CD :
- Build automation : [GRADLE, MAVEN, NPM_SCRIPTS]
- Test execution : [PARALLEL_RUNS, DYNAMIC_SCALING]
- Artifact management : [NEXUS, ARTIFACTORY, DOCKER_REGISTRY]
- Deployment : [KUBERNETES, DOCKER_COMPOSE, CLOUD_RUN]

OUTILS ET FRAMEWORKS :
TESTING FRAMEWORKS :
- Unit : [FRAMEWORK_PAR_LANGAGE]
- Integration : [REST_ASSURED, TESTCONTAINERS]
- E2E : [SELENIUM_GRID, CYPRESS, PLAYWRIGHT]
- Performance : [JMETER, GATLING, K6]
- Security : [OWASP_ZAP, BURP_SUITE]

QUALITY GATES :
- Code quality : [SONARQUBE, CODECLIMATE]
- Security scanning : [SNYK, BLACKDUCK, CHECKMARX]
- Performance baselines : [RESPONSE_TIME_THRESHOLDS]
- Coverage minimums : [BRANCH_COVERAGE_TARGETS]

GESTION DES DONN√âES DE TEST :
- Test data management : [DATA_FACTORIES, FIXTURES]
- Data isolation : [DATABASE_SANDBOXES, API_MOCKING]
- Data generation : [RANDOM_DATA, EDGE_CASES]
- Data cleanup : [AUTOMATED_TEARDOWN, STATE_RESET]

MONITORING ET ANALYTICS :
METRICS DE QUALIT√â :
- Test coverage trends
- Flaky test detection
- Test execution times
- Failure patterns analysis

DASHBOARDS ET REPORTING :
- Real-time dashboards : [GRAFANA, KIBANA]
- Test reports : [ALLURE, EXTENT_REPORTS]
- Trend analysis : [HISTORICAL_PERFORMANCE]
- Stakeholder communication : [EXECUTIVE_SUMMARIES]

CONTINUOUS IMPROVEMENT :
TEST EVOLUTION :
- New test case identification
- Test maintenance automation
- Performance optimization
- False positive reduction

TEAM DYNAMICS :
- Test ownership distribution
- Knowledge sharing sessions
- Pair testing practices
- Community of practice

INNOVATION TESTING :
- Exploratory testing sessions
- AI-assisted test generation
- Visual regression testing
- Accessibility testing automation

SCALING ET MAINTENANCE :
ARCHITECTURE √âVOLUTIVE :
- Test microservices
- Shared test libraries
- Service virtualization
- Cross-team test coordination

COST OPTIMIZATION :
- Cloud cost management
- Test execution optimization
- Maintenance effort reduction
- ROI measurement and tracking

GOVERNANCE ET COMPLIANCE :
- Test standards enforcement
- Regulatory compliance testing
- Audit trails maintenance
- Quality assurance processes
```

## üéØ Templates Sp√©cialis√©s

### Template Tests API RESTful

```
Cr√©e une suite compl√®te de tests API pour [API_REST] avec [FRAMEWORK_TEST].

SP√âCIFICATION API :
- Endpoints : [LISTE_URLS_AVEC_M√âTHODES]
- Authentification : [BEARER_TOKEN, API_KEY, OAUTH]
- Format donn√©es : [JSON, XML, FORM_DATA]

CAT√âGORIES DE TESTS :
FONCTIONNELS :
- CRUD operations : POST, GET, PUT, DELETE
- Status codes validation (200, 201, 400, 401, 404, 500)
- Response schema validation
- Business rules enforcement

S√âCURIT√â :
- Authentication bypass attempts
- Authorization level validation
- Input validation (SQL injection, XSS)
- Rate limiting testing
- CORS configuration

PERFORMANCE :
- Response time under normal load
- Concurrent user simulation
- Memory leak detection
- Timeout handling

INTEGRATION :
- Database state changes
- External service dependencies
- Message queue interactions
- Cache invalidation

INFRASTRUCTURE DE TEST :
- Base URL configuration
- Test data setup/teardown
- Authentication token management
- Response assertion helpers

RAPPORTS ET MONITORING :
- Test execution results
- Coverage reports
- Performance metrics
- Failure analysis
```

### Template Tests Performance et Charge

```
D√©veloppe une strat√©gie de tests de performance pour [APPLICATION] supportant [CHARGE_CIBLE].

OBJECTIFS DE PERFORMANCE :
- Utilisateurs simultan√©s : [NOMBRE_CIBLE]
- Temps de r√©ponse : < [LATENCE_MAX]ms
- Throughput : [REQU√äTES/SECONDE_CIBLE]
- Disponibilit√© : [UPTIME_CIBLE]%

SC√âNARIOS DE TEST :
CHARGE NORMALE :
- Utilisateurs : [X] concurrents
- Dur√©e : [Y] minutes
- Assertions : [M√âTRIQUES_CIBLES]

PIC DE CHARGE :
- Utilisateurs : [X*3] concurrents
- Dur√©e : [Z] minutes
- Assertions : [COMPORTEMENT_ATTENDU]

STRESS TESTING :
- Utilisateurs : [PROGRESSIF_JUSQU'√Ä_LIMITE]
- Monitoring : [RESSOURCES_SYST√àME]
- Breakpoint identification

ENDURANCE TESTING :
- Charge constante : [X] utilisateurs
- Dur√©e : [24H+] heures
- Monitoring : [MEMORY_LEAKS, PERFORMANCE_DEGRADATION]

OUTILS ET INFRASTRUCTURE :
- Load generators : [JMETER, GATLING, K6]
- Monitoring : [PROMETHEUS, DATADOG, NEW_RELIC]
- Infrastructure : [AWS, GCP, AZURE_LOAD_TESTING]
- Reporting : [HTML_REPORTS, METRICS_DASHBOARDS]

ANALYSE DES R√âSULTATS :
- Performance bottlenecks identification
- Scaling recommendations
- Infrastructure optimization
- Code profiling insights

RECOMMANDATIONS D'OPTIMISATION :
- Database query optimization
- Caching strategy implementation
- CDN integration
- Horizontal scaling configuration
```

### Template Tests S√©curit√© Automatis√©s

```
Impl√©mente des tests de s√©curit√© automatis√©s pour [APPLICATION] selon [STANDARD_S√âCURIT√â].

STANDARDS APPLIQU√âS :
- OWASP Top 10 2021
- CIS Controls
- NIST Cybersecurity Framework
- Industry-specific regulations

CAT√âGORIES DE TESTS :
AUTHENTIFICATION :
- Password policy enforcement
- Multi-factor authentication
- Session management security
- Brute force protection

AUTORISATION :
- Role-based access control (RBAC)
- Least privilege principle
- API endpoint authorization
- Data-level security

VALIDATION DES DONN√âES :
- Input sanitization
- SQL injection prevention
- Cross-site scripting (XSS) protection
- Command injection blocking

GESTION DES SESSIONS :
- Session fixation prevention
- Secure cookie configuration
- Session timeout enforcement
- Concurrent session control

CRYPTAGE :
- Data at rest encryption
- Data in transit (TLS 1.3)
- Key management security
- Hashing algorithm validation

INFRASTRUCTURE S√âCURIT√â :
- Firewall configuration
- Intrusion detection
- Log security monitoring
- Backup security validation

OUTILS D'AUTOMATISATION :
- DAST : [OWASP_ZAP, BURP_SUITE, NESSUS]
- SAST : [SONARQUBE, CHECKMARX, VERACODE]
- Container security : [TRIVY, CLAIR, FALCO]
- Infrastructure : [TERRAFORM_SECURITY, CLOUDFORMATION_GUARD]

INT√âGRATION CI/CD :
- Security gates : [FAIL_BUILD_ON_VULNERABILITIES]
- Automated scanning : [EVERY_COMMIT, DAILY, WEEKLY]
- Compliance reporting : [AUDIT_TRAILS, COMPLIANCE_DASHBOARDS]
- Remediation tracking : [VULNERABILITY_MANAGEMENT_SYSTEM]

RECOMMANDATIONS DE REM√âDIATION :
- Prioritized vulnerability fixes
- Security hardening procedures
- Training and awareness programs
- Incident response improvements
```

## üìä M√©triques de Qualit√© des Tests

### Couverture et Efficacit√©

| M√©trique | Excellent | Bon | √Ä Am√©liorer |
|----------|-----------|-----|-------------|
| **Coverage Unit** | > 85% | 70-85% | < 70% |
| **Coverage Integration** | > 60% | 40-60% | < 40% |
| **Coverage E2E** | > 30% | 15-30% | < 15% |
| **Test Execution Time** | < 15 min | 15-45 min | > 45 min |
| **Flaky Tests Rate** | < 1% | 1-5% | > 5% |
| **Defect Leakage** | < 5% | 5-15% | > 15% |

### Maintenance et √âvolutivit√©

| Aspect | Bonnes Pratiques | Indicateurs Succ√®s |
|--------|------------------|-------------------|
| **Test Code Quality** | DRY principle, readable names | Code review approvals > 95% |
| **Data Management** | Isolated test data, automated cleanup | No test data conflicts |
| **CI/CD Integration** | Automated pipelines, fast feedback | Deployment frequency > daily |
| **Team Collaboration** | Shared libraries, documentation | Knowledge sharing sessions |
| **Evolution** | Refactoring, new test addition | Test debt < 10% codebase |

### ROI des Tests Automatis√©s

**Calcul du Retour sur Investissement** :
```
ROI = (√âconomies_tests_manuels + Pr√©vention_d√©fauts + Acc√©l√©ration_livraison) / Co√ªt_automatisation
```

**√âconomies Typiques** :
- Tests de r√©gression : 80% temps √©conomis√©
- D√©bogage production : 60% r√©duction incidents
- Time-to-market : 40% acc√©l√©ration
- Qualit√© per√ßue : +25% satisfaction client

Ces templates constituent votre arsenal complet pour impl√©menter des strat√©gies de test robustes, automatis√©es et maintenables, garantissant la qualit√© et la fiabilit√© de vos applications √† toutes les √©chelles.
