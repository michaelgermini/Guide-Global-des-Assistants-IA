# üìê Annexe A : Formules de R√©gression et Classification

## R√©gression Lin√©aire Simple

### Formule Fondamentale
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx
```
O√π :
- ≈∑ = Valeur pr√©dite (variable d√©pendante)
- Œ≤‚ÇÄ = Ordonn√©e √† l'origine (intercept)
- Œ≤‚ÇÅ = Coefficient de pente (slope)
- x = Variable ind√©pendante

### Calcul des Coefficients par Moindres Carr√©s

**Coefficient de pente Œ≤‚ÇÅ :**
```
Œ≤‚ÇÅ = Œ£[(x·µ¢ - Œº‚Çì)(y·µ¢ - Œº·µß)] / Œ£(x·µ¢ - Œº‚Çì)¬≤
```

**Ordonn√©e √† l'origine Œ≤‚ÇÄ :**
```
Œ≤‚ÇÄ = Œº·µß - Œ≤‚ÇÅŒº‚Çì
```

### M√©triques d'√âvaluation

**Coefficient de d√©termination R¬≤ :**
```
R¬≤ = 1 - (SS_res / SS_tot)
   = Œ£(≈∑·µ¢ - Œº·µß)¬≤ / Œ£(y·µ¢ - Œº·µß)¬≤
```
- R¬≤ ‚àà [0,1] : Plus proche de 1 = meilleur mod√®le
- R¬≤ = 0 : Mod√®le n'explique aucune variance
- R¬≤ = 1 : Mod√®le explique parfaitement les donn√©es

**Erreur quadratique moyenne (MSE) :**
```
MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```

**Racine carr√©e de l'erreur quadratique moyenne (RMSE) :**
```
RMSE = ‚àö[(1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤]
```

---

## R√©gression Lin√©aire Multiple

### Formule G√©n√©rale
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çñx‚Çñ + Œµ
```

### M√©thode des Moindres Carr√©s Ordinaires (OLS)

**Solution matricielle :**
```
Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄY
```

O√π :
- X = Matrice des variables ind√©pendantes (n √ó p)
- Y = Vecteur des valeurs observ√©es (n √ó 1)
- Œ≤ÃÇ = Vecteur des coefficients estim√©s (p √ó 1)

### Tests Statistiques

**Test F pour significativit√© globale :**
```
F = [SSR/k] / [SSE/(n-k-1)]
```
- SSR = Somme des carr√©s de r√©gression
- SSE = Somme des carr√©s des r√©sidus
- k = Nombre de variables ind√©pendantes
- n = Nombre d'observations

**Test t pour chaque coefficient :**
```
t = Œ≤ÃÇ‚±º / SE(Œ≤ÃÇ‚±º)
```
- SE(Œ≤ÃÇ‚±º) = Erreur standard du coefficient j
- Compar√© √† distribution t de Student

### Probl√®mes Courants et Solutions

**Multicolin√©arit√© :**
```
VIF‚±º = 1/(1-R‚±º¬≤)
```
- VIF > 10 : Forte multicolin√©arit√©
- Solution : Supprimer variables corr√©l√©es

**H√©t√©rosc√©dasticit√© :**
- Test de Breusch-Pagan ou White
- Solution : Transformation logarithmique ou WLS

---

## R√©gression Polynomiale

### Formule d'Ordre 2
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤
```

### Formule d'Ordre k
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + ... + Œ≤‚Çñx·µè
```

### Choix de l'Ordre Optimal

**Crit√®re d'information d'Akaike (AIC) :**
```
AIC = 2k + n ln(SSE/n)
```

**Crit√®re d'information bay√©sien (BIC) :**
```
BIC = k ln(n) + n ln(SSE/n)
```

### Validation Crois√©e

**Validation crois√©e k-fold :**
```
Erreur CV = (1/k) Œ£ [Erreur fold·µ¢]
```

---

## R√©gression Logistique Binaire

### Fonction Logit
```
logit(p) = ln(p/(1-p)) = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çñx‚Çñ
```

### Fonction Sigmo√Øde (Probabilit√©)
```
p(y=1|x) = 1 / (1 + e^(-(Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çñx‚Çñ)))
```

### Estimation des Param√®tres

**M√©thode du maximum de vraisemblance :**
- Pas de solution analytique ferm√©e
- Utilisation d'algorithmes it√©ratifs (Newton-Raphson)

### M√©triques d'√âvaluation

**Log-vraisemblance :**
```
LL = Œ£[y·µ¢ ln(pÃÇ·µ¢) + (1-y·µ¢) ln(1-pÃÇ·µ¢)]
```

**D√©viance :**
```
D√©v = -2 LL
```

**Pseudo R¬≤ de McFadden :**
```
R¬≤_McFadden = 1 - (LL_modele / LL_null)
```

---

## Arbres de D√©cision

### Mesure d'Impuret√© (Classification)

**Indice de Gini :**
```
Gini(S) = 1 - Œ£ p·µ¢¬≤
```

**Entropie :**
```
Entropie(S) = - Œ£ p·µ¢ log‚ÇÇ(p·µ¢)
```

### Gain d'Information
```
Gain(S,A) = Entropie(S) - Œ£ (|S·µ•|/|S|) √ó Entropie(S·µ•)
```

### Mesure d'Homog√©n√©it√© (R√©gression)

**Erreur quadratique moyenne :**
```
MSE = (1/n) Œ£(y·µ¢ - Œº_S)¬≤
```

**R√©duction d'impuret√© :**
```
Œî = MSE_parent - Œ£ (n·µ•/n) √ó MSE_enfant
```

### √âlagage (Pruning)

**Co√ªt-complexit√© :**
```
C(T) = SSE(T) + Œ± √ó |T|
```
O√π |T| = nombre de n≈ìuds terminaux

---

## For√™ts Al√©atoires (Random Forest)

### Agr√©gation de Bootstrap (Bagging)

**√âchantillonnage bootstrap :**
- Taille √©chantillon = n (m√™me taille que l'original)
- Probabilit√© de s√©lection = 1/n pour chaque observation
- Remplacement : observations peuvent √™tre s√©lectionn√©es plusieurs fois

### Construction d'Arbre Individuel

**Sous-ensemble de variables :**
- m = ‚àöp pour classification
- m = p/3 pour r√©gression
- O√π p = nombre total de variables

### Pr√©diction Finale

**Classification :**
```
Classe majoritaire = argmax Œ£ I(yÃÇ·µ¢ = classe)
```

**R√©gression :**
```
Pr√©diction moyenne = (1/B) Œ£ ≈∑·µ¢
```

### Importance des Variables

**Decrease in Gini/Accuracy :**
```
Importance·µ• = (1/B) Œ£ Œî·µ¢·µ•
```
O√π Œî·µ¢·µ• = diminution d'impuret√© due √† la variable v dans l'arbre i

---

## SVM (Support Vector Machines)

### Formulation Primal (Cas lin√©aire)
```
Minimize: (1/2)||w||¬≤ + C Œ£ Œæ·µ¢
Subject to: y·µ¢(w¬∑x·µ¢ + b) ‚â• 1 - Œæ·µ¢, Œæ·µ¢ ‚â• 0
```

### Formulation Dual
```
Maximize: Œ£ Œ±·µ¢ - (1/2) Œ£ Œ£ Œ±·µ¢ Œ±‚±º y·µ¢ y‚±º x·µ¢¬∑x‚±º
Subject to: Œ£ Œ±·µ¢ y·µ¢ = 0, 0 ‚â§ Œ±·µ¢ ‚â§ C
```

### Noyau RBF (Radial Basis Function)
```
K(x·µ¢,x‚±º) = exp(-Œ≥||x·µ¢ - x‚±º||¬≤)
```

### Noyau Polynomial
```
K(x·µ¢,x‚±º) = (Œ≥ x·µ¢¬∑x‚±º + r)^d
```

### Param√®tres Critiques

**Param√®tre de r√©gularisation C :**
- C √©lev√© ‚Üí Moins de violations de marge, risque de surapprentissage
- C faible ‚Üí Plus de violations, mod√®le plus g√©n√©ralisable

**Param√®tre Œ≥ (pour RBF) :**
- Œ≥ √©lev√© ‚Üí Fonction de d√©cision tr√®s irr√©guli√®re
- Œ≥ faible ‚Üí Fonction de d√©cision tr√®s lisse

---

## R√©seaux de Neurones

### Neurone Artificiel
```
z = Œ£ w·µ¢ x·µ¢ + b
a = œÉ(z)
```

### Fonctions d'Activation

**Sigmo√Øde :**
```
œÉ(z) = 1/(1 + e^(-z))
```

**ReLU (Rectified Linear Unit) :**
```
ReLU(z) = max(0, z)
```

**Tanh :**
```
tanh(z) = (e^z - e^(-z))/(e^z + e^(-z))
```

### R√©tropropagation (Backpropagation)

**Erreur quadratique :**
```
E = (1/2) Œ£ (y - ≈∑)¬≤
```

**Gradient descent :**
```
w := w - Œ∑ ‚àÇE/‚àÇw
```

### Optimisation

**Adam optimizer :**
```
m_t = Œ≤‚ÇÅ m_(t-1) + (1-Œ≤‚ÇÅ) g_t
v_t = Œ≤‚ÇÇ v_(t-1) + (1-Œ≤‚ÇÇ) g_t¬≤
w_t = w_(t-1) - Œ∑ mÃÇ_t / (‚àövÃÇ_t + Œµ)
```

### R√©gularisation

**Dropout :**
- Probabilit√© p de garder un neurone actif
- Pr√©vention du surapprentissage

**Batch Normalization :**
```
xÃÇ = (x - Œº_batch)/‚àö(œÉ_batch¬≤ + Œµ)
xÃÉ = Œ≥ xÃÇ + Œ≤
```

---

## √âvaluation des Mod√®les

### M√©triques de Classification

**Matrice de Confusion :**
```
               Pr√©dit Positif    Pr√©dit N√©gatif
R√©el Positif         TP                FN
R√©el N√©gatif         FP                TN
```

**Pr√©cision (Precision) :**
```
Precision = TP / (TP + FP)
```

**Rappel (Recall) :**
```
Recall = TP / (TP + FN)
```

**F1-Score :**
```
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**Sp√©cificit√© :**
```
Specificity = TN / (TN + FP)
```

### Courbe ROC et AUC

**Taux de vrais positifs :**
```
TPR = TP / (TP + FN)
```

**Taux de faux positifs :**
```
FPR = FP / (FP + TN)
```

**AUC (Area Under Curve) :**
```
AUC = ‚à´ TPR d(FPR) de 0 √† 1
```

### M√©triques de R√©gression

**Erreur absolue moyenne (MAE) :**
```
MAE = (1/n) Œ£ |y·µ¢ - ≈∑·µ¢|
```

**Erreur quadratique moyenne (MSE) :**
```
MSE = (1/n) Œ£ (y·µ¢ - ≈∑·µ¢)¬≤
```

**Erreur absolue m√©diane (MedAE) :**
```
MedAE = m√©diane(|y·µ¢ - ≈∑·µ¢|)
```

**Coefficient de d√©termination ajust√© :**
```
R¬≤_ajust√© = 1 - [(1-R¬≤)(n-1)] / (n-k-1)
```

---

## Validation Crois√©e

### K-Fold Cross-Validation

**Erreur de validation :**
```
CV_error = (1/K) Œ£ MSE_fold·µ¢
```

### Leave-One-Out Cross-Validation (LOOCV)

**Erreur LOOCV :**
```
LOOCV_error = (1/n) Œ£ MSE_(-i)
```

### Stratified K-Fold

**Pour classification d√©s√©quilibr√©e :**
- Pr√©servation des proportions de classes dans chaque fold
- R√©duction de la variance de l'estimation d'erreur

---

## S√©lection de Mod√®les

### Crit√®res d'Information

**AIC (Akaike Information Criterion) :**
```
AIC = 2k - 2 ln(LÃÇ)
```

**BIC (Bayesian Information Criterion) :**
```
BIC = k ln(n) - 2 ln(LÃÇ)
```

### M√©thodes de R√©gularisation

**Ridge Regression (L2) :**
```
Minimize: Œ£(y·µ¢ - ≈∑·µ¢)¬≤ + Œª Œ£ Œ≤‚±º¬≤
```

**Lasso Regression (L1) :**
```
Minimize: Œ£(y·µ¢ - ≈∑·µ¢)¬≤ + Œª Œ£ |Œ≤‚±º|
```

**Elastic Net :**
```
Minimize: Œ£(y·µ¢ - ≈∑·µ¢)¬≤ + Œª‚ÇÅ Œ£ |Œ≤‚±º| + Œª‚ÇÇ Œ£ Œ≤‚±º¬≤
```

---

## ‚ö†Ô∏è ATTENTION : Interpr√©tation des M√©triques

### **Probl√®mes Courants**

**Overfitting :**
- M√©triques excellentes sur donn√©es d'entra√Ænement
- Performance d√©grad√©e sur nouvelles donn√©es
- Solution : Validation crois√©e, r√©gularisation

**Data Leakage :**
- Information du futur dans donn√©es pass√©es
- M√©triques artificiellement gonfl√©es
- Solution : S√©paration temporelle stricte

**Classes D√©s√©quilibr√©es :**
- M√©triques biais√©es vers classe majoritaire
- Precision/Recall plus informatifs que accuracy
- Solution : M√©triques √©quilibr√©es, r√©√©chantillonnage

### **Bonnes Pratiques**

**Validation crois√©e syst√©matique :**
- Au minimum 5-fold CV
- Stratification pour classification
- R√©p√©titions multiples pour robustesse

**Comparaison de mod√®les :**
- Tests statistiques sur diff√©rences de performance
- Consid√©ration complexit√© vs performance
- Validation sur donn√©es ind√©pendantes

**M√©triques m√©tier pertinentes :**
- Alignement avec objectifs business
- Co√ªts diff√©rentiels des erreurs
- Seuils de d√©cision optimis√©s

---

## üí° CONSEIL : Choix du Mod√®le

### **Guide de S√©lection**

| Type de Probl√®me | Donn√©es | Mod√®le Recommand√© | Justification |
|------------------|---------|-------------------|----------------|
| **Lin√©aire simple** | n<1000, p<10 | R√©gression lin√©aire | Interpr√©table, rapide |
| **Relations complexes** | n>1000 | Random Forest | Robuste, non-param√©trique |
| **Pr√©cision maximale** | Donn√©es propres | Gradient Boosting | Performance sup√©rieure |
| **Interpr√©tabilit√©** | R√©glement√© | R√©gression logistique | Transparente, probabiliste |
| **Donn√©es haute dim.** | p>>n | SVM avec noyau | Efficace en grande dimension |
| **S√©quences/temps** | Donn√©es temporelles | ARIMA, LSTM | Sp√©cialis√© s√©ries temporelles |

### **Consid√©rations Pratiques**

**Temps de calcul :**
- Mod√®les lin√©aires : Formation instantan√©e
- Random Forest : Quelques minutes
- Neural Networks : Heures/jours

**Maintenabilit√© :**
- Mod√®les simples : Faciles √† maintenir
- Mod√®les complexes : Expertise sp√©cialis√©e requise

**Scalabilit√© :**
- Mod√®les lin√©aires : Excellente scalabilit√©
- SVM : Limit√©e √† n~10,000
- Neural Networks : Scalable avec ressources

---

## ‚úì BONNES PRATIQUES : Workflow ML Complet

### **1. Pr√©paration des Donn√©es**
- Analyse exploratoire (EDA)
- Nettoyage et imputation
- Feature engineering
- S√©lection de variables

### **2. S√©lection et Entra√Ænement**
- Baseline simple (r√©gression lin√©aire)
- Comparaison de mod√®les
- Optimisation d'hyperparam√®tres
- Validation crois√©e

### **3. √âvaluation et Interpr√©tation**
- M√©triques appropri√©es au probl√®me
- Analyse d'erreurs
- Interpr√©tabilit√© du mod√®le
- Tests de robustesse

### **4. D√©ploiement et Monitoring**
- Mise en production
- Monitoring performance
- Retraining automatique
- Gestion du drift de donn√©es

### **5. Documentation et Gouvernance**
- Code versionn√©
- M√©tadonn√©es compl√®tes
- Tests automatis√©s
- Conformit√© r√©glementaire